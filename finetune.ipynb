{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf57c7a5-5814-46e9-a3ff-8e5f65024993",
   "metadata": {},
   "source": [
    "# Phase 3: QLoRA Fine-Tuning â€” Qwen2.5-VL-2B on OpenPack\n",
    "\n",
    "ðŸ”— **Live Kaggle Notebook:** [PASTE YOUR KAGGLE URL HERE AFTER RUNNING]\n",
    "\n",
    "Fine-tunes Qwen2.5-VL-2B-Instruct using 4-bit QLoRA on OpenPack packaging operations dataset.\n",
    "\n",
    "**Target compute:** Kaggle 2Ã—T4 (32 GB) or GCP Vertex AI A100 (40 GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc27c52b-fbfd-47ff-826a-3b73f2dfbd73",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m result \u001b[38;5;241m=\u001b[39m subprocess\u001b[38;5;241m.\u001b[39mrun([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia-smi\u001b[39m\u001b[38;5;124m\"\u001b[39m], capture_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mstdout)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import subprocess, torch\n",
    "\n",
    "result = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        total = props.total_memory / 1e9\n",
    "        print(f\"GPU {i}: {props.name} | {total:.1f} GB\")\n",
    "else:\n",
    "    print(\"No GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e9ff93a-e47a-4219-b6a1-a5df4ed15529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All packages installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers==4.41.2 accelerate==0.30.1 peft==0.11.1\n",
    "!pip install -q bitsandbytes==0.43.1 trl==0.8.6\n",
    "!pip install -q decord webdataset einops qwen-vl-utils\n",
    "!pip install -q datasets huggingface-hub openpack-toolkit\n",
    "print(\"âœ“ All packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d47578-22a4-47ba-99c4-85e74d04ca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  Model (4-bit):           2.00 GB\n",
      "  LoRA adapters:           0.30 GB\n",
      "  Activations (raw):       0.01 GB\n",
      "  Activations (+GC 0.4Ã—):  0.01 GB\n",
      "  Optimizer states:        0.60 GB\n",
      "  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "  TOTAL ESTIMATED VRAM:    2.91 GB\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "T4  (16 GB): âœ“ FITS\n",
      "2Ã—T4(32 GB): âœ“ FITS\n",
      "A100(40 GB): âœ“ FITS\n",
      "\n",
      "âœ“ VRAM math passes T4 assertion\n"
     ]
    }
   ],
   "source": [
    "# â”€â”€ REQUIRED VRAM Budget Calculation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "model_base_4bit  = 2.0    # GB â€” Qwen2-VL-2B at 4-bit (2B params Ã— 0.5 bytes)\n",
    "lora_adapters    = 0.3    # GB â€” LoRA rank=16, targeting q/k/v/o projections\n",
    "frames_per_clip  = 8      # Frames sampled per 5-second clip\n",
    "frame_tokens     = 256    # Visual tokens per frame (14Ã—14 patches + merge)\n",
    "batch_size       = 2\n",
    "token_hidden_dim = 1536   # Qwen2-VL-2B hidden size (from config.json)\n",
    "\n",
    "# Raw activation memory\n",
    "activation_gb = (frames_per_clip * frame_tokens * batch_size * token_hidden_dim * 2) / 1e9\n",
    "\n",
    "# With gradient checkpointing: 40% stored (rest recomputed on backward pass)\n",
    "activation_with_gc = activation_gb * 0.4\n",
    "\n",
    "# Optimizer (AdamW): 2 momentum states per LoRA param\n",
    "optimizer_gb = lora_adapters * 2\n",
    "\n",
    "total_vram_gb = model_base_4bit + lora_adapters + activation_with_gc + optimizer_gb\n",
    "\n",
    "print(f\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  Model (4-bit):           {model_base_4bit:.2f} GB\")\n",
    "print(f\"  LoRA adapters:           {lora_adapters:.2f} GB\")\n",
    "print(f\"  Activations (raw):       {activation_gb:.2f} GB\")\n",
    "print(f\"  Activations (+GC 0.4Ã—):  {activation_with_gc:.2f} GB\")\n",
    "print(f\"  Optimizer states:        {optimizer_gb:.2f} GB\")\n",
    "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  TOTAL ESTIMATED VRAM:    {total_vram_gb:.2f} GB\")\n",
    "print(f\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"T4  (16 GB): {'âœ“ FITS' if total_vram_gb < 16 else 'âœ— OOM'}\")\n",
    "print(f\"2Ã—T4(32 GB): {'âœ“ FITS' if total_vram_gb < 32 else 'âœ— OOM'}\")\n",
    "print(f\"A100(40 GB): {'âœ“ FITS' if total_vram_gb < 40 else 'âœ— OOM'}\")\n",
    "\n",
    "assert total_vram_gb < 16.0, f\"Estimate {total_vram_gb:.2f} GB exceeds single T4!\"\n",
    "print(\"\\nâœ“ VRAM math passes T4 assertion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de0c085-9fe8-446a-ae40-116b11781c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config ready. Output dir: /kaggle/working/checkpoints\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name:   str = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    data_root:    str = \"/kaggle/input/openpack-dataset\"   # adjust for GCP\n",
    "    output_dir:   str = \"/kaggle/working/checkpoints\"\n",
    "\n",
    "    # LoRA\n",
    "    lora_rank:    int   = 16\n",
    "    lora_alpha:   int   = 32\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_targets: list  = field(default_factory=lambda: [\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"])\n",
    "\n",
    "    # Training\n",
    "    epochs:       int   = 3\n",
    "    batch_size:   int   = 2\n",
    "    grad_accum:   int   = 8       # effective batch = 16\n",
    "    lr:           float = 2e-4\n",
    "    warmup:       float = 0.05\n",
    "    weight_decay: float = 0.01\n",
    "\n",
    "    # Memory\n",
    "    use_4bit:     bool  = True\n",
    "    grad_ckpt:    bool  = True\n",
    "\n",
    "    # Checkpointing\n",
    "    save_steps:   int   = 50\n",
    "    save_limit:   int   = 3\n",
    "    eval_steps:   int   = 100\n",
    "    log_steps:    int   = 10\n",
    "\n",
    "    # Clip\n",
    "    frames:       int   = 8\n",
    "    max_seq_len:  int   = 2048\n",
    "\n",
    "cfg = Config()\n",
    "Path(cfg.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Config ready. Output dir: {cfg.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e692d5b-775e-4503-b932-1d6b7a3fe65e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Qwen2VLForConditionalGeneration' from 'transformers' (C:\\Users\\Satyam kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 4-bit quantization\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Qwen2VLForConditionalGeneration' from 'transformers' (C:\\Users\\Satyam kumar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# 4-bit quantization\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ") if cfg.use_4bit else None\n",
    "\n",
    "# Load base model\n",
    "print(f\"Loading {cfg.model_name}...\")\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    quantization_config=bnb_cfg,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(cfg.model_name)\n",
    "print(f\"Loaded. Total params: {sum(p.numel() for p in model.parameters())/1e9:.2f}B\")\n",
    "\n",
    "# Prepare for k-bit training (REQUIRED before LoRA)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA\n",
    "lora_cfg = LoraConfig(\n",
    "    r=cfg.lora_rank,\n",
    "    lora_alpha=cfg.lora_alpha,\n",
    "    target_modules=cfg.lora_targets,\n",
    "    lora_dropout=cfg.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_cfg)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Enable gradient checkpointing â€” all 3 flags required together\n",
    "if cfg.grad_ckpt:\n",
    "    model.gradient_checkpointing_enable()    # Flag 1\n",
    "    model.enable_input_require_grads()       # Flag 2 (needed with PEFT)\n",
    "    print(\"âœ“ Gradient checkpointing enabled\")\n",
    "\n",
    "# Show actual VRAM after load\n",
    "if torch.cuda.is_available():\n",
    "    alloc = torch.cuda.memory_allocated() / 1e9\n",
    "    resrv = torch.cuda.memory_reserved()  / 1e9\n",
    "    print(f\"\\nActual VRAM â€” Allocated: {alloc:.2f} GB | Reserved: {resrv:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26052e-2052-4ae0-8245-91e27a47330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/kaggle/working\")   # so data_pipeline.py can be imported\n",
    "\n",
    "from pathlib import Path\n",
    "from data_pipeline import build_hf_dataset, TRAIN_SUBJECTS, VAL_SUBJECTS\n",
    "\n",
    "data_root   = Path(cfg.data_root)\n",
    "frame_cache = Path(\"/kaggle/working/frame_cache\")\n",
    "\n",
    "print(\"Building training dataset...\")\n",
    "train_ds = build_hf_dataset(data_root, TRAIN_SUBJECTS, frame_cache)\n",
    "print(f\"  Train: {len(train_ds)} examples\")\n",
    "\n",
    "print(\"Building validation dataset...\")\n",
    "val_ds = build_hf_dataset(data_root, VAL_SUBJECTS, frame_cache)\n",
    "print(f\"  Val:   {len(val_ds)} examples\")\n",
    "\n",
    "# Sanity check\n",
    "s = train_ds[0]\n",
    "print(f\"\\nSample: {s['clip_id']}\")\n",
    "print(f\"Operation: {s['operation']} â†’ Next: {s['next_operation']}\")\n",
    "print(f\"Turns: {[m['role'] for m in s['messages']]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97df61f-8e72-41e3-8acc-cb48ecc69b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "class Collator:\n",
    "    \"\"\"Qwen2-VL multimodal collator: converts dataset rows to model input batches.\"\"\"\n",
    "    def __init__(self, proc, max_len=2048):\n",
    "        self.proc    = proc\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        texts = []\n",
    "        imgs  = []\n",
    "        for ex in examples:\n",
    "            t = self.proc.apply_chat_template(\n",
    "                ex[\"messages\"], tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(t)\n",
    "            imgs.append(ex.get(\"images\", []))\n",
    "\n",
    "        batch = self.proc(\n",
    "            text=texts,\n",
    "            images=imgs if any(imgs) else None,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = batch[\"input_ids\"].clone()\n",
    "        labels[labels == self.proc.tokenizer.pad_token_id] = -100\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "collator = Collator(processor, max_len=cfg.max_seq_len)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir                  = cfg.output_dir,\n",
    "    per_device_train_batch_size = cfg.batch_size,\n",
    "    gradient_accumulation_steps = cfg.grad_accum,     # effective batch = 16\n",
    "    per_device_eval_batch_size  = 1,\n",
    "    fp16                        = True,\n",
    "    optim                       = \"adamw_torch\",\n",
    "    learning_rate               = cfg.lr,\n",
    "    weight_decay                = cfg.weight_decay,\n",
    "    warmup_ratio                = cfg.warmup,\n",
    "    lr_scheduler_type           = \"cosine\",\n",
    "    num_train_epochs            = cfg.epochs,\n",
    "    gradient_checkpointing      = cfg.grad_ckpt,      # Flag 3\n",
    "    save_strategy               = \"steps\",\n",
    "    save_steps                  = cfg.save_steps,\n",
    "    save_total_limit            = cfg.save_limit,\n",
    "    eval_strategy               = \"steps\",\n",
    "    eval_steps                  = cfg.eval_steps,\n",
    "    logging_steps               = cfg.log_steps,\n",
    "    remove_unused_columns       = False,\n",
    "    report_to                   = \"none\",\n",
    "    seed                        = 42,\n",
    ")\n",
    "\n",
    "print(f\"Effective batch size: {cfg.batch_size * cfg.grad_accum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44270396-7c8d-48bf-91f9-f5609a9c0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Check for existing checkpoint to resume from\n",
    "ckpt_dir   = Path(cfg.output_dir)\n",
    "resume_ckpt = None\n",
    "checkpoints = sorted(ckpt_dir.glob(\"checkpoint-*\"))\n",
    "if checkpoints:\n",
    "    resume_ckpt = str(checkpoints[-1])\n",
    "    print(f\"Resuming from: {resume_ckpt}\")\n",
    "else:\n",
    "    print(\"Starting fresh training\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model         = model,\n",
    "    args          = train_args,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset  = val_ds,\n",
    "    data_collator = collator,\n",
    "    tokenizer     = processor.tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Starting QLoRA fine-tuning...\")\n",
    "result = trainer.train(resume_from_checkpoint=resume_ckpt)\n",
    "\n",
    "# Save final checkpoint\n",
    "final = f\"{cfg.output_dir}/lora_final\"\n",
    "model.save_pretrained(final)\n",
    "processor.save_pretrained(final)\n",
    "print(f\"\\nâœ“ Done! Checkpoint saved â†’ {final}\")\n",
    "print(\"\\nMetrics:\", result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcea91b-2187-47c2-9deb-343b9eabdf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        peak = torch.cuda.max_memory_allocated(i) / 1e9\n",
    "        print(f\"GPU {i} peak: {peak:.2f} GB\")\n",
    "\n",
    "    print(f\"\\nVRAM estimate (Cell 4): {total_vram_gb:.2f} GB\")\n",
    "    ratio = peak / total_vram_gb\n",
    "    print(f\"Ratio actual/estimate:  {ratio:.2f}Ã—\")\n",
    "    status = \"âœ“ Self-consistent\" if ratio < 1.5 else \"âš  Underestimated\"\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb692f3-32f0-4a32-bdc8-ecc053b6d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Create dummy test image\n",
    "test_imgs = [Image.new(\"RGB\", (336, 336), color=(100, 80, 60)) for _ in range(8)]\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": [\n",
    "    *[{\"type\": \"image\", \"image\": im} for im in test_imgs],\n",
    "    {\"type\": \"text\", \"text\":\n",
    "        'Analyze this warehouse packaging video. Reply with JSON: '\n",
    "        '{\"dominant_operation\":\"<op>\",\"temporal_segment\":{\"start_frame\":0,\"end_frame\":0},'\n",
    "        '\"anticipated_next_operation\":\"<op>\",\"confidence\":0.9}'}\n",
    "]}]\n",
    "\n",
    "from qwen_vl_utils import process_vision_info\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "img_inp, vid_inp = process_vision_info(messages)\n",
    "inputs = processor(text=[text], images=img_inp, videos=vid_inp, return_tensors=\"pt\")\n",
    "device = next(model.parameters()).device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n",
    "\n",
    "resp = processor.batch_decode(out[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)[0]\n",
    "print(\"Model response:\\n\", resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67cfcab-22d4-43d1-8226-f199b0220f89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
