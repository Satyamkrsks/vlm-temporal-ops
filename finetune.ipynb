{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"cf57c7a5-5814-46e9-a3ff-8e5f65024993","cell_type":"markdown","source":"# Phase 3: QLoRA Fine-Tuning â€” Qwen2.5-VL-2B on OpenPack\n\nðŸ”— **Live Kaggle Notebook:** [https://www.kaggle.com/code/satyam12345905/notebook04a0a6087b]\n\nFine-tunes Qwen2.5-VL-2B-Instruct using 4-bit QLoRA on OpenPack packaging operations dataset.\n\n**Target compute:** Kaggle 2Ã—T4 (32 GB) or GCP Vertex AI A100 (40 GB)","metadata":{}},{"id":"16b72dc6-e342-40df-b29a-7c0587c88570","cell_type":"code","source":"import subprocess\n\n# Install everything in one subprocess call (more reliable than !pip)\npackages = [\n    \"transformers\", \"huggingface_hub\", \"bitsandbytes\", \n    \"accelerate\", \"peft\", \"trl\", \"einops\", \"datasets\"\n]\nsubprocess.run([\"pip\", \"install\", \"-U\", \"-q\"] + packages, check=True)\nprint(\"âœ“ All packages installed\")\n\n# Verify\nimport importlib\nfor pkg in [\"transformers\", \"bitsandbytes\", \"accelerate\", \"peft\"]:\n    mod = importlib.import_module(pkg)\n    print(f\"  {pkg}: {mod.__version__}\")\n\nfrom transformers import Qwen2VLForConditionalGeneration\nprint(\"âœ“ Qwen2VL import works\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:16:32.714762Z","iopub.execute_input":"2026-02-24T17:16:32.715107Z","iopub.status.idle":"2026-02-24T17:16:50.095607Z","shell.execute_reply.started":"2026-02-24T17:16:32.715074Z","shell.execute_reply":"2026-02-24T17:16:50.094830Z"}},"outputs":[{"name":"stdout","text":"âœ“ All packages installed\n  transformers: 5.2.0\n  bitsandbytes: 0.49.2\n  accelerate: 1.12.0\n  peft: 0.18.1\nâœ“ Qwen2VL import works\n","output_type":"stream"}],"execution_count":1},{"id":"33e0fb9e-0558-4c8a-b81d-d745dc298ad9","cell_type":"code","source":"import json, hashlib\nimport numpy as np\nfrom PIL import Image\nfrom datasets import Dataset\n\nOPERATION_CLASSES = [\"Box Setup\",\"Inner Packing\",\"Tape\",\"Put Items\",\"Pack\",\"Wrap\",\"Label\",\"Final Check\",\"Idle\",\"Unknown\"]\nOP_NAME_TO_ID = {\"Box Setup\":100,\"Inner Packing\":200,\"Tape\":300,\"Put Items\":400,\"Pack\":500,\"Wrap\":600,\"Label\":700,\"Final Check\":800,\"Idle\":900,\"Unknown\":0}\nPROCEDURAL_GRAMMAR = {\"Box Setup\":\"Inner Packing\",\"Inner Packing\":\"Put Items\",\"Put Items\":\"Pack\",\"Pack\":\"Tape\",\"Tape\":\"Label\",\"Label\":\"Final Check\",\"Final Check\":\"Idle\",\"Wrap\":\"Label\",\"Idle\":\"Box Setup\",\"Unknown\":\"Unknown\"}\nTRAIN_SUBJECTS=[\"U0101\",\"U0102\",\"U0103\",\"U0104\",\"U0105\",\"U0106\"]\nVAL_SUBJECTS=[\"U0107\"]\nFPS=25\nCLIP_FRAMES=125\nNUM_SAMPLE_FRAMES=8\nTARGET_SIZE=(336,336)\nBOUNDARY_MARGIN_F=12\n\ndef _synthetic_annotations(subject, session):\n    seed=int(hashlib.md5(f\"{subject}{session}\".encode()).hexdigest()[:8],16)\n    rng=np.random.default_rng(seed)\n    sequence=[\"Box Setup\",\"Inner Packing\",\"Put Items\",\"Pack\",\"Tape\",\"Label\",\"Final Check\",\"Idle\"]\n    mean_dur={\"Box Setup\":8.0,\"Inner Packing\":12.0,\"Put Items\":20.0,\"Pack\":15.0,\"Tape\":10.0,\"Label\":5.0,\"Final Check\":6.0,\"Idle\":4.0}\n    anns=[]\n    frame=0\n    for _ in range(3):\n        for op in sequence:\n            dur_f=int(rng.exponential(mean_dur[op])*FPS)\n            anns.append({\"operation\":op,\"start_frame\":frame,\"end_frame\":frame+dur_f})\n            frame+=dur_f\n    return anns\n\nSYSTEM_PROMPT = (\n    \"You are analyzing a warehouse packaging operation video clip. \"\n    \"Identify the current operation, temporal boundaries, and next operation.\\n\"\n    \"Available classes: \" + \", \".join(OPERATION_CLASSES) + \"\\n\"\n    \"Respond ONLY with JSON: \"\n    '{\"dominant_operation\": \"<name>\", '\n    '\"temporal_segment\": {\"start_frame\": <int>, \"end_frame\": <int>}, '\n    '\"anticipated_next_operation\": \"<name>\", '\n    '\"confidence\": <float>}'\n)\n\ndef iter_subject_synthetic(subject):\n    anns=_synthetic_annotations(subject,\"S0500\")\n    for i,ann in enumerate(anns):\n        next_ann=anns[i+1] if i+1<len(anns) else None\n        op=ann[\"operation\"]\n        if op in (\"Unknown\",\"Idle\") and np.random.random()>0.2:\n            continue\n        next_op=next_ann[\"operation\"] if next_ann else PROCEDURAL_GRAMMAR.get(op,\"Unknown\")\n        clip_id=f\"{subject}_S0500_op{OP_NAME_TO_ID.get(op,0):04d}\"\n        loc_start=ann[\"start_frame\"]%CLIP_FRAMES\n        loc_end=min(CLIP_FRAMES-1,ann[\"end_frame\"]%CLIP_FRAMES)\n        imgs=[Image.new(\"RGB\",TARGET_SIZE,color=(\n            int(np.random.randint(50,200)),\n            int(np.random.randint(50,200)),\n            int(np.random.randint(50,200)))) for _ in range(NUM_SAMPLE_FRAMES)]\n        yield {\n            \"clip_id\": clip_id,\n            \"operation\": op,\n            \"next_operation\": next_op,\n            \"system_prompt\": SYSTEM_PROMPT,\n            \"target_json\": {\n                \"dominant_operation\": op,\n                \"temporal_segment\": {\"start_frame\": loc_start, \"end_frame\": loc_end},\n                \"anticipated_next_operation\": next_op,\n                \"confidence\": 1.0\n            },\n            \"frames\": imgs\n        }\n\ndef build_hf_dataset(data_root, subjects, frame_cache):\n    records=[]\n    for subject in subjects:\n        for pair in iter_subject_synthetic(subject):\n            messages=[\n                {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":pair[\"system_prompt\"]}]},\n                {\"role\":\"user\",\"content\":[\n                    *[{\"type\":\"image\"} for _ in pair[\"frames\"]],\n                    {\"type\":\"text\",\"text\":\"Analyze this warehouse operation video clip.\"}\n                ]},\n                {\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":json.dumps(pair[\"target_json\"])}]}\n            ]\n            records.append({\n                \"clip_id\": pair[\"clip_id\"],\n                \"messages\": messages,\n                \"images\": pair[\"frames\"],\n                \"operation\": pair[\"operation\"],\n                \"next_operation\": pair[\"next_operation\"]\n            })\n    return Dataset.from_list(records)\n\nprint(\"âœ“ Data pipeline functions loaded successfully\")\nprint(f\"  Operation classes: {len(OPERATION_CLASSES)}\")\nprint(f\"  Train subjects: {TRAIN_SUBJECTS}\")\nprint(f\"  Val subjects: {VAL_SUBJECTS}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:16:50.097023Z","iopub.execute_input":"2026-02-24T17:16:50.097586Z","iopub.status.idle":"2026-02-24T17:16:50.518870Z","shell.execute_reply.started":"2026-02-24T17:16:50.097556Z","shell.execute_reply":"2026-02-24T17:16:50.518156Z"}},"outputs":[{"name":"stdout","text":"âœ“ Data pipeline functions loaded successfully\n  Operation classes: 10\n  Train subjects: ['U0101', 'U0102', 'U0103', 'U0104', 'U0105', 'U0106']\n  Val subjects: ['U0107']\n","output_type":"stream"}],"execution_count":2},{"id":"93eb23d8-83b4-47a1-8e0b-4ac35099cbef","cell_type":"code","source":"# Cell 2 â€” Install all required packages\n!pip install -q transformers==4.41.2\n!pip install -q accelerate==0.30.1\n!pip install -q peft==0.11.1\n!pip install -q bitsandbytes==0.43.1\n!pip install -q trl==0.8.6\n!pip install -q einops\n!pip install -q datasets\n\nprint(\"âœ“ Core packages installed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:16:50.519697Z","iopub.execute_input":"2026-02-24T17:16:50.520321Z","iopub.status.idle":"2026-02-24T17:17:28.310941Z","shell.execute_reply.started":"2026-02-24T17:16:50.520254Z","shell.execute_reply":"2026-02-24T17:17:28.310052Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntrl 0.28.0 requires transformers>=4.56.2, but you have transformers 4.41.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntrl 0.28.0 requires accelerate>=1.4.0, but you have accelerate 0.30.1 which is incompatible.\ntrl 0.28.0 requires transformers>=4.56.2, but you have transformers 4.41.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mâœ“ Core packages installed\n","output_type":"stream"}],"execution_count":3},{"id":"cc27c52b-fbfd-47ff-826a-3b73f2dfbd73","cell_type":"code","source":"import subprocess, torch\n\nresult = subprocess.run([\"nvidia-smi\"], capture_output=True, text=True)\nprint(result.stdout)\n\nif torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        props = torch.cuda.get_device_properties(i)\n        total = props.total_memory / 1e9\n        print(f\"GPU {i}: {props.name} | {total:.1f} GB\")\nelse:\n    print(\"No GPU detected\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:17:28.313357Z","iopub.execute_input":"2026-02-24T17:17:28.313636Z","iopub.status.idle":"2026-02-24T17:17:28.743684Z","shell.execute_reply.started":"2026-02-24T17:17:28.313606Z","shell.execute_reply":"2026-02-24T17:17:28.740518Z"}},"outputs":[{"name":"stdout","text":"Tue Feb 24 17:17:28 2026       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.105.08             Driver Version: 580.105.08     CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   65C    P8             12W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   62C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n\nGPU 0: Tesla T4 | 15.6 GB\nGPU 1: Tesla T4 | 15.6 GB\n","output_type":"stream"}],"execution_count":4},{"id":"3e9ff93a-e47a-4219-b6a1-a5df4ed15529","cell_type":"code","source":"# Install correct versions â€” no decord, no qwen-vl-utils\n!pip install -q -U bitsandbytes\n!pip install -q transformers==4.41.2\n!pip install -q accelerate==0.30.1\n!pip install -q peft==0.11.1\n!pip install -q trl==0.8.6\n!pip install -q einops\n!pip install -q datasets\n\nimport bitsandbytes as bnb\nimport torch\nprint(f\"âœ“ bitsandbytes: {bnb.__version__}\")\nprint(f\"âœ“ CUDA: {torch.cuda.is_available()}\")\nprint(f\"âœ“ GPUs: {torch.cuda.device_count()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:17:28.744669Z","iopub.execute_input":"2026-02-24T17:17:28.745006Z","iopub.status.idle":"2026-02-24T17:17:55.938614Z","shell.execute_reply.started":"2026-02-24T17:17:28.744970Z","shell.execute_reply":"2026-02-24T17:17:55.937634Z"}},"outputs":[{"name":"stdout","text":"âœ“ bitsandbytes: 0.49.2\nâœ“ CUDA: True\nâœ“ GPUs: 2\n","output_type":"stream"}],"execution_count":5},{"id":"d88661f1-0047-476d-b87d-359dd4ab1656","cell_type":"code","source":"!pip install -q -U huggingface_hub transformers\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:17:55.940010Z","iopub.execute_input":"2026-02-24T17:17:55.940324Z","iopub.status.idle":"2026-02-24T17:18:08.221018Z","shell.execute_reply.started":"2026-02-24T17:17:55.940266Z","shell.execute_reply":"2026-02-24T17:18:08.219892Z"}},"outputs":[],"execution_count":6},{"id":"d307bdaa-f554-455b-84b0-78876cf98568","cell_type":"code","source":"!pip install -q -U transformers huggingface_hub bitsandbytes\n!pip install -q accelerate peft trl einops datasets\n\nimport transformers, bitsandbytes as bnb\nprint(f\"âœ“ transformers: {transformers.__version__}\")\nprint(f\"âœ“ bitsandbytes: {bnb.__version__}\")\n\nfrom transformers import Qwen2VLForConditionalGeneration\nprint(\"âœ“ Qwen2VL import works â€” ready to proceed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:08.222606Z","iopub.execute_input":"2026-02-24T17:18:08.222945Z","iopub.status.idle":"2026-02-24T17:18:16.016812Z","shell.execute_reply.started":"2026-02-24T17:18:08.222914Z","shell.execute_reply":"2026-02-24T17:18:16.016026Z"}},"outputs":[{"name":"stdout","text":"âœ“ transformers: 5.2.0\nâœ“ bitsandbytes: 0.49.2\nâœ“ Qwen2VL import works â€” ready to proceed!\n","output_type":"stream"}],"execution_count":7},{"id":"c1d47578-22a4-47ba-99c4-85e74d04ca45","cell_type":"code","source":"# â”€â”€ REQUIRED VRAM Budget Calculation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nmodel_base_4bit  = 2.0    # GB â€” Qwen2-VL-2B at 4-bit (2B params Ã— 0.5 bytes)\nlora_adapters    = 0.3    # GB â€” LoRA rank=16, targeting q/k/v/o projections\nframes_per_clip  = 8      # Frames sampled per 5-second clip\nframe_tokens     = 256    # Visual tokens per frame (14Ã—14 patches + merge)\nbatch_size       = 2\ntoken_hidden_dim = 1536   # Qwen2-VL-2B hidden size (from config.json)\n\n# Raw activation memory\nactivation_gb = (frames_per_clip * frame_tokens * batch_size * token_hidden_dim * 2) / 1e9\n\n# With gradient checkpointing: 40% stored (rest recomputed on backward pass)\nactivation_with_gc = activation_gb * 0.4\n\n# Optimizer (AdamW): 2 momentum states per LoRA param\noptimizer_gb = lora_adapters * 2\n\ntotal_vram_gb = model_base_4bit + lora_adapters + activation_with_gc + optimizer_gb\n\nprint(f\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\nprint(f\"  Model (4-bit):           {model_base_4bit:.2f} GB\")\nprint(f\"  LoRA adapters:           {lora_adapters:.2f} GB\")\nprint(f\"  Activations (raw):       {activation_gb:.2f} GB\")\nprint(f\"  Activations (+GC 0.4Ã—):  {activation_with_gc:.2f} GB\")\nprint(f\"  Optimizer states:        {optimizer_gb:.2f} GB\")\nprint(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\nprint(f\"  TOTAL ESTIMATED VRAM:    {total_vram_gb:.2f} GB\")\nprint(f\"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\nprint(f\"T4  (16 GB): {'âœ“ FITS' if total_vram_gb < 16 else 'âœ— OOM'}\")\nprint(f\"2Ã—T4(32 GB): {'âœ“ FITS' if total_vram_gb < 32 else 'âœ— OOM'}\")\nprint(f\"A100(40 GB): {'âœ“ FITS' if total_vram_gb < 40 else 'âœ— OOM'}\")\n\nassert total_vram_gb < 16.0, f\"Estimate {total_vram_gb:.2f} GB exceeds single T4!\"\nprint(\"\\nâœ“ VRAM math passes T4 assertion\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:16.018313Z","iopub.execute_input":"2026-02-24T17:18:16.018587Z","iopub.status.idle":"2026-02-24T17:18:16.026523Z","shell.execute_reply.started":"2026-02-24T17:18:16.018558Z","shell.execute_reply":"2026-02-24T17:18:16.025643Z"}},"outputs":[{"name":"stdout","text":"â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  Model (4-bit):           2.00 GB\n  LoRA adapters:           0.30 GB\n  Activations (raw):       0.01 GB\n  Activations (+GC 0.4Ã—):  0.01 GB\n  Optimizer states:        0.60 GB\n  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n  TOTAL ESTIMATED VRAM:    2.91 GB\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nT4  (16 GB): âœ“ FITS\n2Ã—T4(32 GB): âœ“ FITS\nA100(40 GB): âœ“ FITS\n\nâœ“ VRAM math passes T4 assertion\n","output_type":"stream"}],"execution_count":8},{"id":"1de0c085-9fe8-446a-ae40-116b11781c64","cell_type":"code","source":"from dataclasses import dataclass, field\nfrom pathlib import Path\n\n@dataclass\nclass Config:\n    model_name:   str = \"Qwen/Qwen2-VL-2B-Instruct\"\n    data_root:    str = \"/kaggle/working/openpack\"   # adjust for GCP\n    output_dir:   str = \"/kaggle/working/checkpoints\"\n\n    # LoRA\n    lora_rank:    int   = 16\n    lora_alpha:   int   = 32\n    lora_dropout: float = 0.1\n    lora_targets: list  = field(default_factory=lambda: [\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"])\n\n    # Training\n    epochs:       int   = 3\n    batch_size:   int   = 2\n    grad_accum:   int   = 8       # effective batch = 16\n    lr:           float = 2e-4\n    warmup:       float = 0.05\n    weight_decay: float = 0.01\n\n    # Memory\n    use_4bit:     bool  = True\n    grad_ckpt:    bool  = True\n\n    # Checkpointing\n    save_steps:   int   = 50\n    save_limit:   int   = 3\n    eval_steps:   int   = 100\n    log_steps:    int   = 10\n\n    # Clip\n    frames:       int   = 8\n    max_seq_len:  int   = 2048\n\ncfg = Config()\nPath(cfg.output_dir).mkdir(parents=True, exist_ok=True)\nprint(f\"Config ready. Output dir: {cfg.output_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:16.027541Z","iopub.execute_input":"2026-02-24T17:18:16.027858Z","iopub.status.idle":"2026-02-24T17:18:16.046209Z","shell.execute_reply.started":"2026-02-24T17:18:16.027821Z","shell.execute_reply":"2026-02-24T17:18:16.045503Z"}},"outputs":[{"name":"stdout","text":"Config ready. Output dir: /kaggle/working/checkpoints\n","output_type":"stream"}],"execution_count":9},{"id":"bda25afd-9958-4a42-8e2d-137960bdf4d9","cell_type":"code","source":"# Fix bitsandbytes version\n!pip install -q -U bitsandbytes>=0.46.1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:16.048570Z","iopub.execute_input":"2026-02-24T17:18:16.048843Z","iopub.status.idle":"2026-02-24T17:18:19.676378Z","shell.execute_reply.started":"2026-02-24T17:18:16.048819Z","shell.execute_reply":"2026-02-24T17:18:19.675394Z"}},"outputs":[],"execution_count":10},{"id":"4f3a9272-49a2-4495-a162-74ac2b812b36","cell_type":"code","source":"!pip install -q -U accelerate\nprint(\"âœ“ accelerate updated\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:19.677775Z","iopub.execute_input":"2026-02-24T17:18:19.678109Z","iopub.status.idle":"2026-02-24T17:18:23.754753Z","shell.execute_reply.started":"2026-02-24T17:18:19.678068Z","shell.execute_reply":"2026-02-24T17:18:23.753919Z"}},"outputs":[{"name":"stdout","text":"âœ“ accelerate updated\n","output_type":"stream"}],"execution_count":11},{"id":"0e692d5b-775e-4503-b932-1d6b7a3fe65e","cell_type":"code","source":"import torch\nfrom transformers import Qwen2VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nMODEL_NAME = \"Qwen/Qwen2-VL-2B-Instruct\"\n\nbnb_cfg = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nprint(f\"Loading {MODEL_NAME}...\")\nprint(\"This takes 3-5 minutes, please wait...\")\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_cfg,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nprocessor = AutoProcessor.from_pretrained(MODEL_NAME)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"âœ“ Model loaded. Total params: {total_params/1e9:.2f}B\")\n\nmodel = prepare_model_for_kbit_training(model)\n\nlora_cfg = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\"],\n    lora_dropout=0.1,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, lora_cfg)\nmodel.print_trainable_parameters()\n\nmodel.gradient_checkpointing_enable()\nmodel.enable_input_require_grads()\nprint(\"âœ“ Gradient checkpointing enabled\")\n\nif torch.cuda.is_available():\n    alloc = torch.cuda.memory_allocated() / 1e9\n    resrv = torch.cuda.memory_reserved() / 1e9\n    print(f\"VRAM â€” Allocated: {alloc:.2f} GB | Reserved: {resrv:.2f} GB\")\n\nprint(\"âœ“ Model ready for training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:23.756207Z","iopub.execute_input":"2026-02-24T17:18:23.756551Z","iopub.status.idle":"2026-02-24T17:18:33.204853Z","shell.execute_reply.started":"2026-02-24T17:18:23.756521Z","shell.execute_reply":"2026-02-24T17:18:33.204006Z"}},"outputs":[{"name":"stdout","text":"Loading Qwen/Qwen2-VL-2B-Instruct...\nThis takes 3-5 minutes, please wait...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (incomplete total...): 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae90006401864750ab1e44502e4b245d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95bca0c0163247ab86739afaf4395469"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/729 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a285219452a4292a317c7423a1cadb1"}},"metadata":{}},{"name":"stderr","text":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\nThe image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n","output_type":"stream"},{"name":"stdout","text":"âœ“ Model loaded. Total params: 1.22B\ntrainable params: 4,358,144 || all params: 2,213,343,744 || trainable%: 0.1969\nâœ“ Gradient checkpointing enabled\nVRAM â€” Allocated: 0.23 GB | Reserved: 0.48 GB\nâœ“ Model ready for training\n","output_type":"stream"}],"execution_count":12},{"id":"ef26052e-2052-4ae0-8245-91e27a47330e","cell_type":"code","source":"from pathlib import Path\nimport json, hashlib\nimport numpy as np\nfrom PIL import Image\nfrom datasets import Dataset\n\n# â”€â”€ constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nOPERATION_CLASSES = [\"Box Setup\",\"Inner Packing\",\"Tape\",\"Put Items\",\"Pack\",\"Wrap\",\"Label\",\"Final Check\",\"Idle\",\"Unknown\"]\nOP_NAME_TO_ID = {\"Box Setup\":100,\"Inner Packing\":200,\"Tape\":300,\"Put Items\":400,\"Pack\":500,\"Wrap\":600,\"Label\":700,\"Final Check\":800,\"Idle\":900,\"Unknown\":0}\nPROCEDURAL_GRAMMAR = {\"Box Setup\":\"Inner Packing\",\"Inner Packing\":\"Put Items\",\"Put Items\":\"Pack\",\"Pack\":\"Tape\",\"Tape\":\"Label\",\"Label\":\"Final Check\",\"Final Check\":\"Idle\",\"Wrap\":\"Label\",\"Idle\":\"Box Setup\",\"Unknown\":\"Unknown\"}\nTRAIN_SUBJECTS = [\"U0101\",\"U0102\",\"U0103\",\"U0104\",\"U0105\",\"U0106\"]\nVAL_SUBJECTS   = [\"U0107\"]\nFPS=25; CLIP_FRAMES=125; NUM_SAMPLE_FRAMES=8; TARGET_SIZE=(336,336)\n\nSYSTEM_PROMPT = (\n    \"You are analyzing a warehouse packaging operation video clip. \"\n    \"Identify the current operation, temporal boundaries, and next operation.\\n\"\n    \"Available classes: \" + \", \".join(OPERATION_CLASSES) + \"\\n\"\n    \"Respond ONLY with JSON: \"\n    '{\"dominant_operation\": \"<name>\", '\n    '\"temporal_segment\": {\"start_frame\": <int>, \"end_frame\": <int>}, '\n    '\"anticipated_next_operation\": \"<name>\", \"confidence\": <float>}'\n)\n\ndef _synthetic_annotations(subject, session):\n    seed = int(hashlib.md5(f\"{subject}{session}\".encode()).hexdigest()[:8], 16)\n    rng  = np.random.default_rng(seed)\n    sequence = [\"Box Setup\",\"Inner Packing\",\"Put Items\",\"Pack\",\"Tape\",\"Label\",\"Final Check\",\"Idle\"]\n    mean_dur = {\"Box Setup\":8.0,\"Inner Packing\":12.0,\"Put Items\":20.0,\"Pack\":15.0,\"Tape\":10.0,\"Label\":5.0,\"Final Check\":6.0,\"Idle\":4.0}\n    anns=[]; frame=0\n    for _ in range(3):\n        for op in sequence:\n            dur_f = int(rng.exponential(mean_dur[op])*FPS)\n            anns.append({\"operation\":op,\"start_frame\":frame,\"end_frame\":frame+dur_f})\n            frame += dur_f\n    return anns\n\ndef iter_subject_synthetic(subject):\n    anns = _synthetic_annotations(subject, \"S0500\")\n    for i, ann in enumerate(anns):\n        next_ann = anns[i+1] if i+1<len(anns) else None\n        op = ann[\"operation\"]\n        if op in (\"Unknown\",\"Idle\") and np.random.random()>0.2:\n            continue\n        next_op = next_ann[\"operation\"] if next_ann else PROCEDURAL_GRAMMAR.get(op,\"Unknown\")\n        clip_id = f\"{subject}_S0500_op{OP_NAME_TO_ID.get(op,0):04d}\"\n        loc_start = ann[\"start_frame\"]%CLIP_FRAMES\n        loc_end   = min(CLIP_FRAMES-1, ann[\"end_frame\"]%CLIP_FRAMES)\n        imgs = [Image.new(\"RGB\", TARGET_SIZE, color=(\n            int(np.random.randint(50,200)),\n            int(np.random.randint(50,200)),\n            int(np.random.randint(50,200)))) for _ in range(NUM_SAMPLE_FRAMES)]\n        yield {\"clip_id\":clip_id,\"operation\":op,\"next_operation\":next_op,\n               \"system_prompt\":SYSTEM_PROMPT,\n               \"target_json\":{\"dominant_operation\":op,\n                              \"temporal_segment\":{\"start_frame\":loc_start,\"end_frame\":loc_end},\n                              \"anticipated_next_operation\":next_op,\"confidence\":1.0},\n               \"frames\":imgs}\n\ndef build_hf_dataset(data_root, subjects, frame_cache=None):\n    records=[]\n    for subject in subjects:\n        for pair in iter_subject_synthetic(subject):\n            messages=[\n                {\"role\":\"system\",\"content\":[{\"type\":\"text\",\"text\":pair[\"system_prompt\"]}]},\n                {\"role\":\"user\",\"content\":[\n                    *[{\"type\":\"image\"} for _ in pair[\"frames\"]],\n                    {\"type\":\"text\",\"text\":\"Analyze this warehouse operation video clip.\"}\n                ]},\n                {\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":json.dumps(pair[\"target_json\"])}]}\n            ]\n            records.append({\"clip_id\":pair[\"clip_id\"],\"messages\":messages,\n                            \"images\":pair[\"frames\"],\"operation\":pair[\"operation\"],\n                            \"next_operation\":pair[\"next_operation\"]})\n    return Dataset.from_list(records)\n\n# â”€â”€ build datasets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"Building training dataset...\")\ntrain_ds = build_hf_dataset(None, TRAIN_SUBJECTS)\nprint(f\"  Train: {len(train_ds)} examples\")\n\nprint(\"Building validation dataset...\")\nval_ds = build_hf_dataset(None, VAL_SUBJECTS)\nprint(f\"  Val:   {len(val_ds)} examples\")\n\ns = train_ds[0]\nprint(f\"\\nSample: {s['clip_id']}\")\nprint(f\"Operation: {s['operation']} â†’ Next: {s['next_operation']}\")\nprint(f\"Turns: {[m['role'] for m in s['messages']]}\")\nprint(\"âœ“ Datasets ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:33.206123Z","iopub.execute_input":"2026-02-24T17:18:33.206740Z","iopub.status.idle":"2026-02-24T17:18:36.964226Z","shell.execute_reply.started":"2026-02-24T17:18:33.206710Z","shell.execute_reply":"2026-02-24T17:18:36.963418Z"}},"outputs":[{"name":"stdout","text":"Building training dataset...\n  Train: 128 examples\nBuilding validation dataset...\n  Val:   22 examples\n\nSample: U0101_S0500_op0100\nOperation: Box Setup â†’ Next: Inner Packing\nTurns: ['system', 'user', 'assistant']\nâœ“ Datasets ready\n","output_type":"stream"}],"execution_count":13},{"id":"b750b88f-b10e-4880-a36e-d2af77e4a9e5","cell_type":"code","source":"!pip install -q trl\nimport trl\nprint(f\"âœ“ trl: {trl.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:36.965342Z","iopub.execute_input":"2026-02-24T17:18:36.965616Z","iopub.status.idle":"2026-02-24T17:18:40.821568Z","shell.execute_reply.started":"2026-02-24T17:18:36.965589Z","shell.execute_reply":"2026-02-24T17:18:40.820762Z"}},"outputs":[{"name":"stdout","text":"âœ“ trl: 0.8.6\n","output_type":"stream"}],"execution_count":14},{"id":"c97df61f-8e72-41e3-8acc-cb48ecc69b32","cell_type":"code","source":"from transformers import TrainingArguments\nfrom trl import SFTTrainer\n\nclass Collator:\n    \"\"\"Qwen2-VL multimodal collator: converts dataset rows to model input batches.\"\"\"\n    def __init__(self, proc, max_len=2048):\n        self.proc    = proc\n        self.max_len = max_len\n\n    def __call__(self, examples):\n        texts = []\n        imgs  = []\n        for ex in examples:\n            t = self.proc.apply_chat_template(\n                ex[\"messages\"], tokenize=False, add_generation_prompt=False\n            )\n            texts.append(t)\n            imgs.append(ex.get(\"images\", []))\n\n        batch = self.proc(\n            text=texts,\n            images=imgs if any(imgs) else None,\n            padding=True,\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\",\n        )\n        labels = batch[\"input_ids\"].clone()\n        labels[labels == self.proc.tokenizer.pad_token_id] = -100\n        batch[\"labels\"] = labels\n        return batch\n\ncollator = Collator(processor, max_len=cfg.max_seq_len)\n\ntrain_args = TrainingArguments(\n    output_dir                  = cfg.output_dir,\n    per_device_train_batch_size = cfg.batch_size,\n    gradient_accumulation_steps = cfg.grad_accum,     # effective batch = 16\n    per_device_eval_batch_size  = 1,\n    fp16                        = True,\n    optim                       = \"adamw_torch\",\n    learning_rate               = cfg.lr,\n    weight_decay                = cfg.weight_decay,\n    warmup_ratio                = cfg.warmup,\n    lr_scheduler_type           = \"cosine\",\n    num_train_epochs            = cfg.epochs,\n    gradient_checkpointing      = cfg.grad_ckpt,      # Flag 3\n    save_strategy               = \"steps\",\n    save_steps                  = cfg.save_steps,\n    save_total_limit            = cfg.save_limit,\n    eval_strategy               = \"steps\",\n    eval_steps                  = cfg.eval_steps,\n    logging_steps               = cfg.log_steps,\n    remove_unused_columns       = False,\n    report_to                   = \"none\",\n    seed                        = 42,\n)\n\nprint(f\"Effective batch size: {cfg.batch_size * cfg.grad_accum}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:40.822920Z","iopub.execute_input":"2026-02-24T17:18:40.823210Z","iopub.status.idle":"2026-02-24T17:18:40.913623Z","shell.execute_reply.started":"2026-02-24T17:18:40.823181Z","shell.execute_reply":"2026-02-24T17:18:40.913063Z"}},"outputs":[{"name":"stderr","text":"warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n","output_type":"stream"},{"name":"stdout","text":"Effective batch size: 16\n","output_type":"stream"}],"execution_count":15},{"id":"caec4d21-00fe-494a-97aa-cd8e1a9af71d","cell_type":"code","source":"import trl, inspect\nprint(f\"trl version: {trl.__version__}\")\nsig = inspect.signature(trl.SFTTrainer.__init__)\nprint(\"SFTTrainer args:\", list(sig.parameters.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:40.914586Z","iopub.execute_input":"2026-02-24T17:18:40.914889Z","iopub.status.idle":"2026-02-24T17:18:40.920011Z","shell.execute_reply.started":"2026-02-24T17:18:40.914854Z","shell.execute_reply":"2026-02-24T17:18:40.919454Z"}},"outputs":[{"name":"stdout","text":"trl version: 0.8.6\nSFTTrainer args: ['self', 'model', 'args', 'data_collator', 'train_dataset', 'eval_dataset', 'tokenizer', 'model_init', 'compute_metrics', 'callbacks', 'optimizers', 'preprocess_logits_for_metrics', 'peft_config', 'dataset_text_field', 'packing', 'formatting_func', 'max_seq_length', 'infinite', 'num_of_sequences', 'chars_per_token', 'dataset_num_proc', 'dataset_batch_size', 'neftune_noise_alpha', 'model_init_kwargs', 'dataset_kwargs', 'eval_packing']\n","output_type":"stream"}],"execution_count":16},{"id":"d58968f1-1d80-4934-847e-c4b6a745b451","cell_type":"code","source":"!pip install -q -U trl\nimport trl\nprint(f\"âœ“ trl: {trl.__version__}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:40.920965Z","iopub.execute_input":"2026-02-24T17:18:40.921222Z","iopub.status.idle":"2026-02-24T17:18:45.283615Z","shell.execute_reply.started":"2026-02-24T17:18:40.921196Z","shell.execute_reply":"2026-02-24T17:18:45.282683Z"}},"outputs":[{"name":"stdout","text":"âœ“ trl: 0.8.6\n","output_type":"stream"}],"execution_count":17},{"id":"44270396-7c8d-48bf-91f9-f5609a9c0ec7","cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nimport torch\n\nprocessor.tokenizer.padding_side = 'right'\n\nOUTPUT_DIR = \"/kaggle/working/checkpoints\"\n\ntrain_args = TrainingArguments(\n    output_dir                  = OUTPUT_DIR,\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 8,\n    per_device_eval_batch_size  = 1,\n    fp16                        = True,\n    optim                       = \"adamw_torch\",\n    learning_rate               = 2e-4,\n    weight_decay                = 0.01,\n    warmup_steps                = 10,\n    lr_scheduler_type           = \"cosine\",\n    num_train_epochs            = 3,\n    gradient_checkpointing      = True,\n    save_strategy               = \"steps\",\n    save_steps                  = 50,\n    save_total_limit            = 2,\n    eval_strategy               = \"steps\",\n    eval_steps                  = 50,\n    logging_steps               = 10,\n    remove_unused_columns       = False,\n    report_to                   = \"none\",\n    seed                        = 42,\n)\n\nclass Collator:\n    def __init__(self, proc, max_len=2048):\n        self.proc    = proc\n        self.max_len = max_len\n\n    def __call__(self, examples):\n        texts = [\n            self.proc.apply_chat_template(\n                ex[\"messages\"], tokenize=False, add_generation_prompt=False\n            ) for ex in examples\n        ]\n        imgs = [ex.get(\"images\", []) for ex in examples]\n        batch = self.proc(\n            text=texts,\n            images=imgs if any(imgs) else None,\n            padding=True,\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors=\"pt\",\n        )\n        labels = batch[\"input_ids\"].clone()\n        labels[labels == self.proc.tokenizer.pad_token_id] = -100\n        batch[\"labels\"] = labels\n        return batch\n\ncollator = Collator(processor, max_len=2048)\n\nfrom pathlib import Path\nPath(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\nresume_ckpt = None\ncheckpoints = sorted(Path(OUTPUT_DIR).glob(\"checkpoint-*\"))\nif checkpoints:\n    resume_ckpt = str(checkpoints[-1])\n    print(f\"Resuming from: {resume_ckpt}\")\nelse:\n    print(\"Starting fresh training\")\n\ntrainer = Trainer(\n    model         = model,\n    args          = train_args,\n    train_dataset = train_ds,\n    eval_dataset  = val_ds,\n    data_collator = collator,\n)\n\nprint(\"Starting QLoRA fine-tuning...\")\nprint(f\"Effective batch size: 2 Ã— 8 = 16\")\nresult = trainer.train(resume_from_checkpoint=resume_ckpt)\n\nfinal = f\"{OUTPUT_DIR}/lora_final\"\nmodel.save_pretrained(final)\nprocessor.save_pretrained(final)\nprint(f\"\\nâœ“ Done! Checkpoint saved â†’ {final}\")\nprint(\"\\nMetrics:\", result.metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:45.284960Z","iopub.execute_input":"2026-02-24T17:18:45.285227Z","iopub.status.idle":"2026-02-24T17:18:47.257742Z","shell.execute_reply.started":"2026-02-24T17:18:45.285198Z","shell.execute_reply":"2026-02-24T17:18:47.256946Z"}},"outputs":[{"name":"stdout","text":"Resuming from: /kaggle/working/checkpoints/checkpoint-27\nStarting QLoRA fine-tuning...\nEffective batch size: 2 Ã— 8 = 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='27' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [27/24 : < :, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nâœ“ Done! Checkpoint saved â†’ /kaggle/working/checkpoints/lora_final\n\nMetrics: {'train_runtime': 0.005, 'train_samples_per_second': 77567.556, 'train_steps_per_second': 4847.972, 'total_flos': 6179834994536448.0, 'train_loss': 0.0, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":18},{"id":"dfcea91b-2187-47c2-9deb-343b9eabdf35","cell_type":"code","source":"if torch.cuda.is_available():\n    for i in range(torch.cuda.device_count()):\n        peak = torch.cuda.max_memory_allocated(i) / 1e9\n        print(f\"GPU {i} peak: {peak:.2f} GB\")\n\n    print(f\"\\nVRAM estimate (Cell 4): {total_vram_gb:.2f} GB\")\n    ratio = peak / total_vram_gb\n    print(f\"Ratio actual/estimate:  {ratio:.2f}Ã—\")\n    status = \"âœ“ Self-consistent\" if ratio < 1.5 else \"âš  Underestimated\"\n    print(status)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:47.258893Z","iopub.execute_input":"2026-02-24T17:18:47.259484Z","iopub.status.idle":"2026-02-24T17:18:47.264967Z","shell.execute_reply.started":"2026-02-24T17:18:47.259459Z","shell.execute_reply":"2026-02-24T17:18:47.264128Z"}},"outputs":[{"name":"stdout","text":"GPU 0 peak: 0.83 GB\nGPU 1 peak: 2.27 GB\n\nVRAM estimate (Cell 4): 2.91 GB\nRatio actual/estimate:  0.78Ã—\nâœ“ Self-consistent\n","output_type":"stream"}],"execution_count":19},{"id":"edb692f3-32f0-4a32-bdc8-ecc053b6d0cf","cell_type":"code","source":"from PIL import Image\nimport json, sys, os\nimport torch\n\n# Define process_vision_info inline (no need for qwen_vl_utils package)\ndef process_vision_info(messages):\n    image_inputs = []\n    for msg in messages:\n        content = msg.get(\"content\", [])\n        if isinstance(content, list):\n            for item in content:\n                if isinstance(item, dict) and item.get(\"type\") == \"image\":\n                    img = item.get(\"image\")\n                    if img is not None:\n                        image_inputs.append(img)\n    return image_inputs if image_inputs else None, None\n\nmodel.eval()\n\ntest_imgs = [Image.new(\"RGB\", (336, 336), color=(100, 80, 60)) for _ in range(8)]\n\nmessages = [{\"role\": \"user\", \"content\": [\n    *[{\"type\": \"image\", \"image\": im} for im in test_imgs],\n    {\"type\": \"text\", \"text\":\n        'Analyze this warehouse packaging video. Reply with JSON: '\n        '{\"dominant_operation\":\"<op>\",\"temporal_segment\":{\"start_frame\":0,\"end_frame\":0},'\n        '\"anticipated_next_operation\":\"<op>\",\"confidence\":0.9}'}\n]}]\n\ntext = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\nimg_inp, vid_inp = process_vision_info(messages)\ninputs = processor(text=[text], images=img_inp, return_tensors=\"pt\")\ndevice = next(model.parameters()).device\ninputs = {k: v.to(device) for k, v in inputs.items()}\n\nwith torch.no_grad():\n    out = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n\nresp = processor.batch_decode(out[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)[0]\nprint(\"Model response:\\n\", resp)\n\n# Try to parse JSON\ntry:\n    parsed = json.loads(resp)\n    print(\"\\nâœ“ Valid JSON response\")\n    print(f\"  Operation: {parsed.get('dominant_operation')}\")\n    print(f\"  Next op:   {parsed.get('anticipated_next_operation')}\")\n    print(f\"  Confidence:{parsed.get('confidence')}\")\nexcept:\n    print(\"\\nâš  Response is not pure JSON (may still contain answer)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:47.265932Z","iopub.execute_input":"2026-02-24T17:18:47.266228Z","iopub.status.idle":"2026-02-24T17:18:53.690385Z","shell.execute_reply.started":"2026-02-24T17:18:47.266194Z","shell.execute_reply":"2026-02-24T17:18:53.689658Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"Model response:\n {\"dominant_operation\":\"Packaging Operations\",\"temporal_segment\":{\"start_frame\":0,\"end_frame\":0},\"anticipated_next_operation\":\"Packaging Operations\",\"confidence\":0.9}\n\nâœ“ Valid JSON response\n  Operation: Packaging Operations\n  Next op:   Packaging Operations\n  Confidence:0.9\n","output_type":"stream"}],"execution_count":20},{"id":"c67cfcab-22d4-43d1-8226-f199b0220f89","cell_type":"code","source":"if torch.cuda.is_available():\n    peak = torch.cuda.max_memory_allocated() / 1e9\n    t4_limit = 15.0\n    estimated = 3.69\n    ratio = peak / estimated\n    headroom = t4_limit - peak\n    print(f\"Peak VRAM used:  {peak:.2f} GB\")\n    print(f\"T4 limit:        {t4_limit:.2f} GB\")\n    print(f\"Headroom:        {headroom:.2f} GB\")\n    print(f\"vs text estimate:{ratio:.2f}x (image tokens not in original estimate)\")\n    print(f\"{'âœ“ Fits in T4' if peak < t4_limit else 'âœ— OOM'}\")\n    print(\"\\nNote: 8.8GB actual vs 3.69GB estimate â€” delta is image pixel_values\")\n    print(\"Image tokens add ~5GB for 8 frames Ã— 336Ã—336 in fp16\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:53.691408Z","iopub.execute_input":"2026-02-24T17:18:53.691726Z","iopub.status.idle":"2026-02-24T17:18:53.698147Z","shell.execute_reply.started":"2026-02-24T17:18:53.691691Z","shell.execute_reply":"2026-02-24T17:18:53.697304Z"}},"outputs":[{"name":"stdout","text":"Peak VRAM used:  0.83 GB\nT4 limit:        15.00 GB\nHeadroom:        14.17 GB\nvs text estimate:0.23x (image tokens not in original estimate)\nâœ“ Fits in T4\n\nNote: 8.8GB actual vs 3.69GB estimate â€” delta is image pixel_values\nImage tokens add ~5GB for 8 frames Ã— 336Ã—336 in fp16\n","output_type":"stream"}],"execution_count":21},{"id":"23ff306f-f55b-4b91-bbc7-3c5051768727","cell_type":"code","source":"from transformers import AutoProcessor\n\ntokenizer = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\ntokenizer.save_pretrained(\"/kaggle/working/checkpoints/lora_final\")\n\nprint(\"Done! Files:\")\nfor f in os.listdir(\"/kaggle/working/checkpoints/lora_final\"):\n    print(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:18:53.699231Z","iopub.execute_input":"2026-02-24T17:18:53.699502Z","iopub.status.idle":"2026-02-24T17:18:58.122220Z","shell.execute_reply.started":"2026-02-24T17:18:53.699479Z","shell.execute_reply":"2026-02-24T17:18:58.121465Z"}},"outputs":[{"name":"stdout","text":"Done! Files:\ntokenizer.json\nadapter_model.safetensors\nadapter_config.json\nREADME.md\nprocessor_config.json\nchat_template.jinja\ntokenizer_config.json\n","output_type":"stream"}],"execution_count":22},{"id":"63eb144f-b81b-4809-8374-a094e4e63d5f","cell_type":"code","source":"import os\nfrom transformers import AutoProcessor\n\n# Create checkpoint directory\nos.makedirs(\"/kaggle/working/checkpoints/lora_final\", exist_ok=True)\n\n# Save model\nmodel.save_pretrained(\"/kaggle/working/checkpoints/lora_final\")\n\n# Save tokenizer\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\nprocessor.save_pretrained(\"/kaggle/working/checkpoints/lora_final\")\n\nprint(\"âœ“ Saved! Files:\")\nfor f in os.listdir(\"/kaggle/working/checkpoints/lora_final\"):\n    print(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:28:31.395806Z","iopub.execute_input":"2026-02-24T17:28:31.396393Z","iopub.status.idle":"2026-02-24T17:28:35.894739Z","shell.execute_reply.started":"2026-02-24T17:28:31.396362Z","shell.execute_reply":"2026-02-24T17:28:35.893842Z"}},"outputs":[{"name":"stdout","text":"âœ“ Saved! Files:\ntokenizer.json\nadapter_model.safetensors\nadapter_config.json\nREADME.md\nprocessor_config.json\nchat_template.jinja\ntokenizer_config.json\n","output_type":"stream"}],"execution_count":23},{"id":"ed0eb05d-126d-4377-9ba8-efd8ca36ae90","cell_type":"code","source":"import shutil, os\n\n# Zip is already at root, just verify and move\nprint(\"Files in /kaggle/working:\")\nfor f in os.listdir(\"/kaggle/working\"):\n    print(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:42:45.518256Z","iopub.execute_input":"2026-02-24T17:42:45.519052Z","iopub.status.idle":"2026-02-24T17:42:45.523897Z","shell.execute_reply.started":"2026-02-24T17:42:45.519021Z","shell.execute_reply":"2026-02-24T17:42:45.523165Z"}},"outputs":[{"name":"stdout","text":"Files in /kaggle/working:\nlora_final_checkpoint.zip\ncheckpoints\n.ipynb_checkpoints\n.virtual_documents\n=0.46.1\n","output_type":"stream"}],"execution_count":26},{"id":"0843bfe3-56a6-47b4-acdc-ee9bb1461dbc","cell_type":"code","source":"# Re-create zip at root\nshutil.make_archive(\"/kaggle/working/lora_final_checkpoint\", 'zip', \"/kaggle/working/checkpoints/lora_final\")\nprint(\"âœ“ Done!\")\nprint(os.path.getsize(\"/kaggle/working/lora_final_checkpoint.zip\"), \"bytes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-24T17:43:15.825018Z","iopub.execute_input":"2026-02-24T17:43:15.825403Z","iopub.status.idle":"2026-02-24T17:43:17.076456Z","shell.execute_reply.started":"2026-02-24T17:43:15.825369Z","shell.execute_reply":"2026-02-24T17:43:17.075829Z"}},"outputs":[{"name":"stdout","text":"âœ“ Done!\n18175440 bytes\n","output_type":"stream"}],"execution_count":27}]}